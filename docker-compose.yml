version: '3.3'

services:

  #SPARK EXAMPLE APPLICATION
  spark-example:
    build: .
    image: flaviostutz/spark-sample
    environment:
      - SPARK_MASTER_NAME=spark-master
      - HDFS_URL=hdfs://namenode1:8020
    networks:
      - hdfs

  #SPARK SERVICES
  spark-master:
    image: bde2020/spark-master:2.4.5-hadoop2.7
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    networks:
      - hdfs

  spark-worker:
    image: bde2020/spark-worker:2.4.5-hadoop2.7
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    networks:
      - hdfs

  #HDFS SERVICES
  namenode1:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    ports:
      - "50070:50070"
      - "9870:9870"
    environment:
      - CLUSTER_NAME=example1
      - INIT_DAEMON_STEP=setup_hdfs
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_disk_balancer_enabled=false
    #   - CORE_CONF_hadoop_http_staticuser_user=root
    #   - CORE_CONF_hadoop_proxyuser_hue_hosts=*
    #   - CORE_CONF_hadoop_proxyuser_hue_groups=*
    volumes:
      - ./volumes/namenode1:/hadoop/dfs/name
    networks:
      - hdfs

  namenode2:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    ports:
      - "50071:50070"
      - "9871:9870"
    environment:
      - CLUSTER_NAME=example1
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
    volumes:
      - ./volumes/namenode2:/hadoop/dfs/name
    networks:
      - hdfs

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode1:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
    ports:
      - "50075:50075"
      - "9864:9864"
    volumes:
      - ./volumes/datanode1:/hadoop/dfs/data
    networks:
      - hdfs

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode1:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
    ports:
      - "50076:50075"
      - "9865:9864"
    volumes:
      - ./volumes/datanode2:/hadoop/dfs/data
    networks:
      - hdfs

  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode1:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
    ports:
      - "50077:50075"
      - "9866:9864"
    volumes:
      - ./volumes/datanode3:/hadoop/dfs/data
    networks:
      - hdfs

  #TOOLS
#   filebrowser:
#     image: bde2020/hdfs-filebrowser
#     ports:
#       - "8088:8088"
#     environment:
#       - CORE_CONF_fs_defaultFS=hdfs://namenode1:8020
#     networks:
#       - hdfs

  #YARN MAP REDUCE SERVICES
#   resourcemanager:
#     image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
#     hostname: resourcemanager
#     expose:
#       - "8031"
#       - "8088"
#     environment:
#       VIRTUAL_HOST: hdfs-resourcemanager.demo.big-data-europe.local
#       VIRTUAL_PORT: 8088
#     env_file:
#       - ./config/hadoop/hadoop.env

#   historyserver:
#     image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
#     hostname: historyserver
#     volumes:
#       - ./data/hadoop/historyserver:/hadoop/yarn/timeline
#     env_file:
#       - ./config/hadoop/hadoop.env

#   nodemanager:
#     image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
#     hostname: nodemanager
#     expose:
#       - "8042"
#     environment:
#       VIRTUAL_HOST: hdfs-nodemanager.demo.big-data-europe.local
#     env_file:
#       - ./config/hadoop/hadoop.env

networks:
  hdfs:

